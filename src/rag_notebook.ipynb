{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from utils import populate_source_files\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(filepath: str) -> List[Document]:\n",
    "    \"\"\"Extract the information from PDF files.\"\"\"\n",
    "    loader = PyPDFLoader(filepath)\n",
    "    docs = loader.load()\n",
    "    logging.info(\"Successfully loaded the doc: %s\", filepath)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "    print(f\"Splitting Doc into {len(splits)} chunks\")\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_function(model_name: str) -> HuggingFaceEmbeddings:\n",
    "    \"\"\"Embedding function to use for embedding the vector tokens.\"\"\"\n",
    "    return HuggingFaceEmbeddings(model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_db(root_folder: Path) -> Chroma:\n",
    "    \"\"\"Load the vector database and return the chroma instance.\"\"\"\n",
    "    files = populate_source_files(root_folder)\n",
    "    documents: List[Document] = []\n",
    "    for f in files:\n",
    "        logging.info(\"Loading %s\", f)\n",
    "        split_documents = load_file(f)\n",
    "        documents.extend(split_documents)\n",
    "    vectorstore = Chroma.from_documents(documents=documents, embedding=get_embedding_function(model_name=\"all-MiniLM-L6-v2\"))\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db = load_db(Path(\"/Users/pshah/Projects/llm-rag-playground\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = vector_db.similarity_search(\"How much speedup has been achieved between 2003 and 2011?\", k = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "os.environ[\"OPENAI_API_KEY\"]=\"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\"\n",
    "model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = \"\\n\\n\".join(a.page_content for a in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from typing import Final\n",
    "\n",
    "PROMPT_TEMPLATE: Final = \"\"\"\n",
    "You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt=PromptTemplate(input_variables=[\"question\", \"context\"], template=PROMPT_TEMPLATE)\n",
    "llm_chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain.invoke({\"question\": \"How much speedup has been achieved between 2003 and 2011?\", \"context\": b})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
